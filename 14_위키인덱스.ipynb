{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867ab257",
   "metadata": {},
   "source": [
    "# Wikipedia 한국어 데이터셋 다운로드 및 벡터 인덱싱\n",
    "\n",
    "이 노트북에서는 Hugging Face Datasets에서 한국어 Wikipedia 데이터셋을 다운로드하고, Pinecone 벡터 데이터베이스에 인덱싱하는 과정을 진행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33996c",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 설치 및 임포트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 및 임포트\n",
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea319476",
   "metadata": {},
   "source": [
    "## 2. Wikipedia 한국어 데이터셋 다운로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia 한국어 데이터셋 다운로드\n",
    "print(\"=== Wikipedia 한국어 데이터셋 다운로드 시작 ===\")\n",
    "\n",
    "try:\n",
    "    # 데이터셋 로드 (처음에는 시간이 오래 걸릴 수 있습니다)\n",
    "    print(\"데이터셋 로딩 중... (처음 실행 시 다운로드가 필요합니다)\")\n",
    "    dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.ko\")\n",
    "    \n",
    "    print(f\"✅ 데이터셋 로드 완료!\")\n",
    "    print(f\"📊 데이터셋 정보:\")\n",
    "    print(f\"   - 훈련 데이터: {len(dataset['train']):,}개 문서\")\n",
    "    print(f\"   - 컬럼: {dataset['train'].column_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {e}\")\n",
    "    dataset = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터 확인\n",
    "if dataset:\n",
    "    print(\"📝 샘플 문서들:\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        sample = dataset['train'][i]\n",
    "        print(f\"\\n--- 문서 {i+1} ---\")\n",
    "        print(f\"ID: {sample.get('id', 'N/A')}\")\n",
    "        print(f\"제목: {sample.get('title', 'N/A')}\")\n",
    "        print(f\"텍스트 길이: {len(sample.get('text', '')):,}자\")\n",
    "        print(f\"텍스트 미리보기: {sample.get('text', '')[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3016a",
   "metadata": {},
   "source": [
    "## 3. 샘플 데이터 저장 및 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터를 로컬에 저장\n",
    "if dataset:\n",
    "    print(\"=== 샘플 데이터 저장 중 ===\")\n",
    "    \n",
    "    # 1000개 샘플 추출\n",
    "    num_samples = 1000\n",
    "    sample_data = dataset['train'].select(range(min(num_samples, len(dataset['train']))))\n",
    "    \n",
    "    # CSV로 저장\n",
    "    sample_data.to_csv('wikipedia_ko_sample.csv')\n",
    "    print(f\"✅ CSV 파일 저장 완료: wikipedia_ko_sample.csv ({len(sample_data)}개 문서)\")\n",
    "    \n",
    "    # JSON으로 저장\n",
    "    sample_data.to_json('wikipedia_ko_sample.json')\n",
    "    print(f\"✅ JSON 파일 저장 완료: wikipedia_ko_sample.json ({len(sample_data)}개 문서)\")\n",
    "    \n",
    "    # 통계 정보\n",
    "    print(f\"\\n📊 샘플 데이터 통계:\")\n",
    "    print(f\"   - 총 문서 수: {len(sample_data):,}개\")\n",
    "    print(f\"   - 평균 텍스트 길이: {sum(len(doc['text']) for doc in sample_data) // len(sample_data):,}자\")\n",
    "    print(f\"   - 최대 텍스트 길이: {max(len(doc['text']) for doc in sample_data):,}자\")\n",
    "    print(f\"   - 최소 텍스트 길이: {min(len(doc['text']) for doc in sample_data):,}자\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 주제로 필터링 예제\n",
    "if dataset:\n",
    "    print(\"=== 특정 주제로 필터링 ===\")\n",
    "    \n",
    "    # '인공지능' 관련 문서 찾기\n",
    "    ai_docs = []\n",
    "    for i, doc in enumerate(tqdm(dataset['train'], desc=\"검색 중...\")):\n",
    "        if '인공지능' in doc.get('title', '') or '인공지능' in doc.get('text', ''):\n",
    "            ai_docs.append(doc)\n",
    "            if len(ai_docs) >= 5:  # 5개만 찾기\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n🤖 '인공지능' 관련 문서 {len(ai_docs)}개 발견:\")\n",
    "    for i, doc in enumerate(ai_docs):\n",
    "        print(f\"{i+1}. {doc.get('title', 'N/A')}\")\n",
    "        print(f\"   텍스트 길이: {len(doc.get('text', '')):,}자\")\n",
    "        print(f\"   미리보기: {doc.get('text', '')[:100]}...\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e54725",
   "metadata": {},
   "source": [
    "## 4. OpenAI API 설정 및 임베딩 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b63e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 설정\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"텍스트를 임베딩으로 변환\"\"\"\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"임베딩 생성 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "# 테스트\n",
    "test_embedding = get_embedding(\"안녕하세요 테스트입니다\")\n",
    "if test_embedding:\n",
    "    print(f\"✅ 임베딩 테스트 성공! 차원: {len(test_embedding)}\")\n",
    "else:\n",
    "    print(\"❌ 임베딩 테스트 실패\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b907f3",
   "metadata": {},
   "source": [
    "## 5. Pinecone 벡터 데이터베이스 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd84a66",
   "metadata": {},
   "source": [
    "## 6. Wikipedia 데이터를 벡터 데이터베이스에 인덱싱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f092c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia 데이터를 벡터로 변환하고 Pinecone에 저장\n",
    "if dataset and test_embedding:\n",
    "    print(\"=== Wikipedia 데이터 벡터화 및 인덱싱 시작 ===\")\n",
    "    \n",
    "    # 샘플 데이터로 시작 (처음에는 작은 규모로 테스트)\n",
    "    sample_size = 100\n",
    "    sample_docs = dataset['train'].select(range(min(sample_size, len(dataset['train']))))\n",
    "    \n",
    "    vectors_to_upsert = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(sample_docs, desc=\"벡터화 중...\")):\n",
    "        # 텍스트 전처리 (너무 긴 텍스트는 잘라내기)\n",
    "        text = doc.get('text', '')\n",
    "        if len(text) > 8000:  # OpenAI 임베딩 모델의 토큰 제한 고려\n",
    "            text = text[:8000]\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        embedding = get_embedding(text)\n",
    "        \n",
    "        if embedding:\n",
    "            vectors_to_upsert.append({\n",
    "                'id': f\"wiki_{doc.get('id', i)}\",\n",
    "                'values': embedding,\n",
    "                'metadata': {\n",
    "                    'title': doc.get('title', ''),\n",
    "                    'text_length': len(doc.get('text', '')),\n",
    "                    'source': 'wikipedia_ko'\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"✅ {len(vectors_to_upsert)}개 벡터 생성 완료\")\n",
    "    \n",
    "    # Pinecone에 벡터 저장\n",
    "    if vectors_to_upsert:\n",
    "        print(\"Pinecone에 벡터 저장 중...\")\n",
    "        index.upsert(vectors=vectors_to_upsert)\n",
    "        print(\"✅ 벡터 저장 완료!\")\n",
    "        \n",
    "        # 인덱스 통계 확인\n",
    "        stats = index.describe_index_stats()\n",
    "        print(f\"📊 인덱스 통계: {stats}\")\n",
    "    else:\n",
    "        print(\"❌ 저장할 벡터가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0cd3a",
   "metadata": {},
   "source": [
    "## 7. 벡터 검색 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 검색 테스트\n",
    "def search_wikipedia(query, top_k=5):\n",
    "    \"\"\"Wikipedia 데이터에서 유사한 문서 검색\"\"\"\n",
    "    # 쿼리를 임베딩으로 변환\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    if not query_embedding:\n",
    "        print(\"❌ 쿼리 임베딩 생성 실패\")\n",
    "        return None\n",
    "    \n",
    "    # Pinecone에서 검색\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 검색 테스트\n",
    "test_queries = [\n",
    "    \"인공지능의 역사\",\n",
    "    \"한국의 전통 음식\",\n",
    "    \"과학 기술 발전\",\n",
    "    \"경제 성장과 발전\"\n",
    "]\n",
    "\n",
    "print(\"=== 벡터 검색 테스트 ===\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\n🔍 검색 쿼리: '{query}'\")\n",
    "    results = search_wikipedia(query, top_k=3)\n",
    "    \n",
    "    if results and results.matches:\n",
    "        for i, match in enumerate(results.matches):\n",
    "            print(f\"  {i+1}. {match.metadata.get('title', 'N/A')} (유사도: {match.score:.3f})\")\n",
    "            print(f\"     텍스트 길이: {match.metadata.get('text_length', 0):,}자\")\n",
    "    else:\n",
    "        print(\"  검색 결과가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c3526",
   "metadata": {},
   "source": [
    "## 8. RAG 시스템 구축을 위한 데이터셋 사용법\n",
    "\n",
    "💡 이제 Wikipedia 한국어 데이터셋을 사용할 수 있습니다:\n",
    "\n",
    "### 1. 전체 데이터셋 사용\n",
    "```python\n",
    "dataset = load_dataset('wikimedia/wikipedia', '20231101.ko')\n",
    "docs = dataset['train']\n",
    "```\n",
    "\n",
    "### 2. 특정 문서 접근\n",
    "```python\n",
    "doc = dataset['train'][0]  # 첫 번째 문서\n",
    "title = doc['title']\n",
    "text = doc['text']\n",
    "```\n",
    "\n",
    "### 3. 샘플 데이터 사용\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('wikipedia_ko_sample.csv')\n",
    "```\n",
    "\n",
    "### 4. RAG 시스템에 활용\n",
    "- 텍스트를 청크로 분할\n",
    "- 임베딩 생성\n",
    "- 벡터 데이터베이스에 저장\n",
    "- 검색 및 생성에 활용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wiki' index 생성 - 1536 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0704f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5b1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3d2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"wiki\"\n",
    "for idx in pc.list_indexes():\n",
    "    # 인덱스 이름이 \"wiki\"와 일치하는 경우 해당 인덱스를 삭제합니다.\n",
    "    if idx.name == index_name:\n",
    "        pc.delete_index(idx.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421c7ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"wiki\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"wiki-506yx3r.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 1536,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536,  # 모델 차원\n",
    "    metric=\"cosine\",  # 모델 메트릭\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c22b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\prompting_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'wiki' 인덱스를 가져옵니다.\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4d920",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a35751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559f2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"wikimedia/wikipedia\", \"20231101.ko\", split='train[:100]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c795d47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f67e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5',\n",
       " 'url': 'https://ko.wikipedia.org/wiki/%EC%A7%80%EB%AF%B8%20%EC%B9%B4%ED%84%B0',\n",
       " 'title': '지미 카터',\n",
       " 'text': '제임스 얼 카터 주니어(, 1924년 10월 1일~)는 민주당 출신 미국의 제39대 대통령(1977년~1981년)이다.\\n\\n생애\\n\\n어린 시절 \\n지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\\n\\n조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\n\\n정계 입문 \\n1962년 조지아주 상원 의원 선거에서 낙선하였으나, 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주지사 선거에 낙선하지만, 1970년 조지아 주지사 선거에서 당선됐다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n\\n대통령 재임 \\n\\n1976년 미합중국 제39대 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워서, 많은 지지를 받았는데 제럴드 포드 대통령을 누르고 당선되었다.\\n\\n카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\\n\\n외교 정책 \\n카터는 이집트와 이스라엘을 조정하여 캠프 데이비드에서 안와르 사다트 대통령과 메나헴 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 체결했다. 이것은 공화당과 미국의 유대인 단체의 반발을 일으켰다. 그러나 1979년, 양국 간의 평화조약이 백악관에서 이루어졌다.\\n\\n소련과 제2차 전략 무기 제한 협상(SALT II)에 조인했다.\\n\\n카터는 1970년대 후반 당시 대한민국 등 인권 후진국의 국민들의 인권을 지키기 위해 노력했으며, 취임 이후 계속해서 도덕정치를 내세웠다.\\n\\n임기 말, 소련의 아프가니스탄 침공 사건으로 인해 1980년 하계 올림픽에 반공국가들의 보이콧을 하였다.\\n\\n그는 주이란 미국 대사관 인질 사건의 인질 구출 실패로 인한 원인으로, 1980년 제40대 대통령 선거에서 공화당의 로널드 레이건에게 패하며 재선에 실패하였다.\\n\\n대한민국과의 관계\\n\\n지미 카터는 한국과의 관계에서도 중요한 영향을 미쳤던 대통령 중 하나다. 인권 문제와 주한미군 철수 문제로 한때 한미 관계가 불편하기도 했다. 1978년 대한민국에 대한 북한의 위협에 대비해 한미연합사를 창설하면서, 1982년까지 3단계에 걸쳐 주한미군을 철수하기로 했다. 그러나 주한미군사령부와 정보기관·의회의 반대에 부딪혀 주한미군은 완전철수 대신 6,000명을 감축하는 데 그쳤다. 또한 박정희 정권의 인권 문제 등과의 논란으로 불협화음을 냈으나, 1979년 6월 하순, 대한민국을 방문했는데 관계가 다소 회복되었다.\\n1979년~1980년 대한민국의 정치적 격변기 당시의 대통령이었던 그는 이에 대해 애매한 태도를 보였고, 이는 후에 대한민국 내에서 고조되는 반미 운동의 한 원인이 됐다. 10월 26일, 박정희 대통령이 김재규 중앙정보부장에 의해 살해된 것에 대해 그는 이 사건으로 큰 충격을 받았으며, 사이러스 밴스 국무장관을 조문사절로 파견했다. 12·12 군사 반란과 5.17 쿠데타에 대해 초기에는 강하게 비난했으나, 미국 정부가 신군부를 설득하는데, 한계가 있었고 결국 묵인하는 듯한 태도를 보이게 됐다.\\n\\n퇴임 이후 \\n\\n퇴임 이후 민간 자원을 적극 활용한 비영리 기구인 카터 재단을 설립한 뒤 민주주의 실현을 위해 제 3세계의 선거 감시 활동 및 기니 벌레에 의한 드라쿤쿠르스 질병 방재를 위해 힘썼다. 미국의 빈곤층 지원 활동, 사랑의 집짓기 운동, 국제 분쟁 중재 등의 활동도 했다.\\n\\n카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 선행하는 행위에 대해 깊은 유감을 표시 하며 미국의 군사적 활동에 강한 반대 입장을 보이고 있다.\\n\\n특히 국제 분쟁 조정을 위해 북한의 김일성, 아이티의 세드라스 장군, 팔레인스타인의 하마스, 보스니아의 세르비아계 정권 같이 미국 정부에 대해 협상을 거부하면서 사태의 위기를 초래한 인물 및 단체를 직접 만나 분쟁의 원인을 근본적으로 해결하기 위해 힘썼다. 이 과정에서 미국 행정부와 갈등을 보이기도 했지만, 전직 대통령의 권한과 재야 유명 인사들의 활약으로 해결해 나갔다.\\n\\n1978년에 채결된 캠프데이비드 협정의 이행이 지지부진 하자 중동 분쟁 분제를 해결하기 위해 1993년 퇴임 후 직접 이스라엘과 팔레인스타인의 오슬로 협정을 이끌어 내는 데도 성공했다.\\n\\n1993년 1차 북핵 위기 당시 북한에 대한 미국의 군사적 행동이 임박했으나, 미국 전직 대통령으로는 처음으로 북한을 방문하고 미국과 북 양국의 중재에 큰 기여를 해 위기를 해결했다는 평가를 받았다. 또한 이 때 김영삼 대통령과 김일성 주석의 만남을 주선했다. 하지만 그로부터 수주일 후 김일성이 갑자기 사망하였는데 김일성과 김영삼의 정상회담은 이루어지지 못했다.\\n\\n미국의 관타나모 수용소 문제, 세계의 인권문제에서도 관심이 깊어 유엔에 유엔인권고등판무관의 제도를 시행하도록 노력하여 독재자들의 인권 유린에 대해 제약을 하고, 국제형사재판소를 만드는데 기여하여 독재자들 같은 인권유린범죄자를 재판소로 회부하여 국제적인 처벌을 받게 하는 등 인권 신장에 크나 큰 기여를 했다.\\n2011년 4월 26일부터 29일까지 북한을 3일간 방문했다.\\n\\n평가 \\n경제문제를 해결하지 못하고 주 이란 미국 대사관 인질 사건에 발목이 잡혀 실패한 대통령으로 평가를 받지만 이란 사태는 미국 내 이란 재산을 풀어주겠다는 조건을 내세워서 사실상 카터가 해결한 것이었고, 사랑의 집짓기 운동 등으로 퇴임 후에 훨씬 더 존경받는 미국 대통령 중에 특이한 인물로 남았다.\\n\\n그는 2002년 말 인권과 중재 역할에 대한 공로를 인정받아 노벨 평화상을 받게 되었다.\\n\\n이외에도, 그는 대통령 재임 시절은 물론 퇴임 후에도 지속적으로 여러 장기 집권중인 독재자들을 만나왔는데, 그와 만난 독재자들 중 절대 다수가 얼마 되지 않아 최후를 맞이하게 되며 \\'독재자의 사신\\'이라는 별명이 붙기도 했다.\\n\\n같이 보기 \\n 주한 미군의 철수\\n 한반도 평화협정\\n\\n역대 선거 결과\\n\\n각주\\n\\n참고 문헌 \\n 《진정한 리더는 떠난 후에 아름답다》 저자 : 지미 카터\\n 《지미 카터》 저자 : 지미 카터(지식의날개, 2018)\\n\\n외부 링크 \\n \\n \\n\\n \\n1924년 출생\\n1976년 미국 대통령 후보\\n1980년 미국 대통령 후보\\n그래미상 수상자\\n노벨 평화상 수상자\\n미국 해군의 장교\\n미국의 침례교도\\n미국의 노벨상 수상자\\n미국의 농부\\n미국의 대통령\\n미국의 역사 (1964-1980)\\n미국의 외교관\\n미국의 인도주의자\\n미국의 제2차 세계 대전 참전 군인\\n미국의 진보주의\\n민주당 (미국)의 정치인\\n살아있는 사람\\n스코틀랜드계 미국인\\n아일랜드계 미국인\\n잉글랜드계 미국인\\n영국계 미국인\\n네덜란드계 미국인\\n스위스계 미국인\\n프랑스계 미국인\\n조지아 공과대학교 동문\\n조지아주의 정치인\\n조지아주지사\\n미국의 회고록 작가\\n에모리 대학교 교수\\n미국 해군사관학교 동문\\n미국 버지니아 종합군사학원 동문\\n미국 미주리 종합군사학원 동문\\n타임 올해의 인물\\n군사 기술자\\n이란 혁명 관련자\\n미국의 민주주의 운동가\\n주이란 미국 대사관 인질 사건\\n조지아주의 민주당 당원\\n조지아주 출신 작가\\n소련-아프가니스탄 전쟁 관련자\\n20세기 미국 사람\\n21세기 미국 사람'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496d4dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b216daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "for rec in data:\n",
    "  if len(rec['text'])>35000:\n",
    "    print('**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99e80e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001CD5CEBBA40>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001CD5EA37590>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1fe40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x1cd5ee85760>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=400,\n",
    "  chunk_overlap =20,\n",
    "  length_function = len, \\\n",
    "  separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  #자르는 순서 문자\n",
    ")\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07aacf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def calculate_metadata_size(metadata):\n",
    "  return len(json.dumps(metadata, ensure_ascii=False).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aabf205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import time\n",
    "texts = []\n",
    "metas = []\n",
    "count = 0\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(data):\n",
    "  full_text = sample['text'] #전문\n",
    "  \n",
    "  #메타데이터 구성\n",
    "  metadata = {\n",
    "    \"wiki_id\": str(sample['id']),\n",
    "    \"url\": sample['url'],\n",
    "    \"title\": sample['title'] #[:100]\n",
    "  }\n",
    "  \n",
    "  # 스플릿터로 자르기\n",
    "  chunks = splitter.split_text(full_text)\n",
    "  #print(len(chunks))\n",
    "  \n",
    "  for i, chunk in enumerate(chunks):\n",
    "    record = {\n",
    "      'chunk_id':i,\n",
    "      'full_text':full_text,\n",
    "      **metadata\n",
    "    }\n",
    "    metadata_size = calculate_metadata_size(record)\n",
    "    if metadata_size > 35000:\n",
    "      continue\n",
    "    \n",
    "    texts.append(chunk)\n",
    "    metas.append(record)\n",
    "    count += 1\n",
    "    \n",
    "    # 청크 배치의 임베딩 -> 적재\n",
    "    if count % batch_size == 0:\n",
    "      ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "      embeddings = embedding.embed_documents(texts) #20개 청크의 임베딩 수행\n",
    "      index.upsert(\n",
    "        vectors=zip(ids, embeddings, metas),\n",
    "        namespace='wiki-ns1'\n",
    "      )\n",
    "      texts=[]\n",
    "      metas=[]\n",
    "      time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
