{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wikipedia 한국어 데이터셋 다운로드\n",
        "\n",
        "이 노트북에서는 Hugging Face Datasets에서 한국어 Wikipedia 데이터셋을 다운로드하고 분석합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치 및 임포트\n",
        "%pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wikipedia 한국어 데이터셋 다운로드\n",
        "print(\"=== Wikipedia 한국어 데이터셋 다운로드 시작 ===\")\n",
        "\n",
        "try:\n",
        "    # 데이터셋 로드 (처음에는 시간이 오래 걸릴 수 있습니다)\n",
        "    print(\"데이터셋 로딩 중... (처음 실행 시 다운로드가 필요합니다)\")\n",
        "    dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.ko\")\n",
        "    \n",
        "    print(f\"✅ 데이터셋 로드 완료!\")\n",
        "    print(f\"📊 데이터셋 정보:\")\n",
        "    print(f\"   - 훈련 데이터: {len(dataset['train']):,}개 문서\")\n",
        "    print(f\"   - 컬럼: {dataset['train'].column_names}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ 오류 발생: {e}\")\n",
        "    dataset = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 샘플 데이터 확인\n",
        "if dataset:\n",
        "    print(\"📝 샘플 문서들:\")\n",
        "    \n",
        "    for i in range(3):\n",
        "        sample = dataset['train'][i]\n",
        "        print(f\"\\n--- 문서 {i+1} ---\")\n",
        "        print(f\"ID: {sample.get('id', 'N/A')}\")\n",
        "        print(f\"제목: {sample.get('title', 'N/A')}\")\n",
        "        print(f\"텍스트 길이: {len(sample.get('text', '')):,}자\")\n",
        "        print(f\"텍스트 미리보기: {sample.get('text', '')[:300]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 샘플 데이터를 로컬에 저장\n",
        "if dataset:\n",
        "    print(\"=== 샘플 데이터 저장 중 ===\")\n",
        "    \n",
        "    # 1000개 샘플 추출\n",
        "    num_samples = 1000\n",
        "    sample_data = dataset['train'].select(range(min(num_samples, len(dataset['train']))))\n",
        "    \n",
        "    # CSV로 저장\n",
        "    sample_data.to_csv('wikipedia_ko_sample.csv')\n",
        "    print(f\"✅ CSV 파일 저장 완료: wikipedia_ko_sample.csv ({len(sample_data)}개 문서)\")\n",
        "    \n",
        "    # JSON으로 저장\n",
        "    sample_data.to_json('wikipedia_ko_sample.json')\n",
        "    print(f\"✅ JSON 파일 저장 완료: wikipedia_ko_sample.json ({len(sample_data)}개 문서)\")\n",
        "    \n",
        "    # 통계 정보\n",
        "    print(f\"\\n📊 샘플 데이터 통계:\")\n",
        "    print(f\"   - 총 문서 수: {len(sample_data):,}개\")\n",
        "    print(f\"   - 평균 텍스트 길이: {sum(len(doc['text']) for doc in sample_data) // len(sample_data):,}자\")\n",
        "    print(f\"   - 최대 텍스트 길이: {max(len(doc['text']) for doc in sample_data):,}자\")\n",
        "    print(f\"   - 최소 텍스트 길이: {min(len(doc['text']) for doc in sample_data):,}자\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 특정 주제로 필터링 예제\n",
        "if dataset:\n",
        "    print(\"=== 특정 주제로 필터링 ===\")\n",
        "    \n",
        "    # '인공지능' 관련 문서 찾기\n",
        "    ai_docs = []\n",
        "    for i, doc in enumerate(tqdm(dataset['train'], desc=\"검색 중...\")):\n",
        "        if '인공지능' in doc.get('title', '') or '인공지능' in doc.get('text', ''):\n",
        "            ai_docs.append(doc)\n",
        "            if len(ai_docs) >= 5:  # 5개만 찾기\n",
        "                break\n",
        "    \n",
        "    print(f\"\\n🤖 '인공지능' 관련 문서 {len(ai_docs)}개 발견:\")\n",
        "    for i, doc in enumerate(ai_docs):\n",
        "        print(f\"{i+1}. {doc.get('title', 'N/A')}\")\n",
        "        print(f\"   텍스트 길이: {len(doc.get('text', '')):,}자\")\n",
        "        print(f\"   미리보기: {doc.get('text', '')[:100]}...\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 데이터셋 사용법\n",
        "\n",
        "💡 이제 Wikipedia 한국어 데이터셋을 사용할 수 있습니다:\n",
        "\n",
        "### 1. 전체 데이터셋 사용\n",
        "```python\n",
        "dataset = load_dataset('wikimedia/wikipedia', '20231101.ko')\n",
        "docs = dataset['train']\n",
        "```\n",
        "\n",
        "### 2. 특정 문서 접근\n",
        "```python\n",
        "doc = dataset['train'][0]  # 첫 번째 문서\n",
        "title = doc['title']\n",
        "text = doc['text']\n",
        "```\n",
        "\n",
        "### 3. 샘플 데이터 사용\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('wikipedia_ko_sample.csv')\n",
        "```\n",
        "\n",
        "### 4. RAG 시스템에 활용\n",
        "- 텍스트를 청크로 분할\n",
        "- 임베딩 생성\n",
        "- 벡터 데이터베이스에 저장\n",
        "- 검색 및 생성에 활용\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
