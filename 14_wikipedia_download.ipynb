{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” Hugging Face Datasetsì—ì„œ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
        "%pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
        "print(\"=== Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ===\")\n",
        "\n",
        "try:\n",
        "    # ë°ì´í„°ì…‹ ë¡œë“œ (ì²˜ìŒì—ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
        "    print(\"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤)\")\n",
        "    dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.ko\")\n",
        "    \n",
        "    print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "    print(f\"   - í›ˆë ¨ ë°ì´í„°: {len(dataset['train']):,}ê°œ ë¬¸ì„œ\")\n",
        "    print(f\"   - ì»¬ëŸ¼: {dataset['train'].column_names}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    dataset = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
        "if dataset:\n",
        "    print(\"ğŸ“ ìƒ˜í”Œ ë¬¸ì„œë“¤:\")\n",
        "    \n",
        "    for i in range(3):\n",
        "        sample = dataset['train'][i]\n",
        "        print(f\"\\n--- ë¬¸ì„œ {i+1} ---\")\n",
        "        print(f\"ID: {sample.get('id', 'N/A')}\")\n",
        "        print(f\"ì œëª©: {sample.get('title', 'N/A')}\")\n",
        "        print(f\"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample.get('text', '')):,}ì\")\n",
        "        print(f\"í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {sample.get('text', '')[:300]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì €ì¥\n",
        "if dataset:\n",
        "    print(\"=== ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì¤‘ ===\")\n",
        "    \n",
        "    # 1000ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
        "    num_samples = 1000\n",
        "    sample_data = dataset['train'].select(range(min(num_samples, len(dataset['train']))))\n",
        "    \n",
        "    # CSVë¡œ ì €ì¥\n",
        "    sample_data.to_csv('wikipedia_ko_sample.csv')\n",
        "    print(f\"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: wikipedia_ko_sample.csv ({len(sample_data)}ê°œ ë¬¸ì„œ)\")\n",
        "    \n",
        "    # JSONìœ¼ë¡œ ì €ì¥\n",
        "    sample_data.to_json('wikipedia_ko_sample.json')\n",
        "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: wikipedia_ko_sample.json ({len(sample_data)}ê°œ ë¬¸ì„œ)\")\n",
        "    \n",
        "    # í†µê³„ ì •ë³´\n",
        "    print(f\"\\nğŸ“Š ìƒ˜í”Œ ë°ì´í„° í†µê³„:\")\n",
        "    print(f\"   - ì´ ë¬¸ì„œ ìˆ˜: {len(sample_data):,}ê°œ\")\n",
        "    print(f\"   - í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {sum(len(doc['text']) for doc in sample_data) // len(sample_data):,}ì\")\n",
        "    print(f\"   - ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´: {max(len(doc['text']) for doc in sample_data):,}ì\")\n",
        "    print(f\"   - ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´: {min(len(doc['text']) for doc in sample_data):,}ì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# íŠ¹ì • ì£¼ì œë¡œ í•„í„°ë§ ì˜ˆì œ\n",
        "if dataset:\n",
        "    print(\"=== íŠ¹ì • ì£¼ì œë¡œ í•„í„°ë§ ===\")\n",
        "    \n",
        "    # 'ì¸ê³µì§€ëŠ¥' ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°\n",
        "    ai_docs = []\n",
        "    for i, doc in enumerate(tqdm(dataset['train'], desc=\"ê²€ìƒ‰ ì¤‘...\")):\n",
        "        if 'ì¸ê³µì§€ëŠ¥' in doc.get('title', '') or 'ì¸ê³µì§€ëŠ¥' in doc.get('text', ''):\n",
        "            ai_docs.append(doc)\n",
        "            if len(ai_docs) >= 5:  # 5ê°œë§Œ ì°¾ê¸°\n",
        "                break\n",
        "    \n",
        "    print(f\"\\nğŸ¤– 'ì¸ê³µì§€ëŠ¥' ê´€ë ¨ ë¬¸ì„œ {len(ai_docs)}ê°œ ë°œê²¬:\")\n",
        "    for i, doc in enumerate(ai_docs):\n",
        "        print(f\"{i+1}. {doc.get('title', 'N/A')}\")\n",
        "        print(f\"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(doc.get('text', '')):,}ì\")\n",
        "        print(f\"   ë¯¸ë¦¬ë³´ê¸°: {doc.get('text', '')[:100]}...\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë°ì´í„°ì…‹ ì‚¬ìš©ë²•\n",
        "\n",
        "ğŸ’¡ ì´ì œ Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "### 1. ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
        "```python\n",
        "dataset = load_dataset('wikimedia/wikipedia', '20231101.ko')\n",
        "docs = dataset['train']\n",
        "```\n",
        "\n",
        "### 2. íŠ¹ì • ë¬¸ì„œ ì ‘ê·¼\n",
        "```python\n",
        "doc = dataset['train'][0]  # ì²« ë²ˆì§¸ ë¬¸ì„œ\n",
        "title = doc['title']\n",
        "text = doc['text']\n",
        "```\n",
        "\n",
        "### 3. ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('wikipedia_ko_sample.csv')\n",
        "```\n",
        "\n",
        "### 4. RAG ì‹œìŠ¤í…œì— í™œìš©\n",
        "- í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
        "- ì„ë² ë”© ìƒì„±\n",
        "- ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
        "- ê²€ìƒ‰ ë° ìƒì„±ì— í™œìš©\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
