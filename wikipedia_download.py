# Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

from datasets import load_dataset
import os

def download_wikipedia_ko():
    """
    Hugging Face Datasetsì—ì„œ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print("=== Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ===")
    
    try:
        # Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ
        print("ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        dataset = load_dataset("wikimedia/wikipedia", "20231101.ko")
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   - í›ˆë ¨ ë°ì´í„°: {len(dataset['train'])}ê°œ ë¬¸ì„œ")
        print(f"   - ì»¬ëŸ¼: {dataset['train'].column_names}")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print(f"\nğŸ“ ìƒ˜í”Œ ë¬¸ì„œ (ì²« ë²ˆì§¸):")
        sample = dataset['train'][0]
        print(f"   - ID: {sample.get('id', 'N/A')}")
        print(f"   - ì œëª©: {sample.get('title', 'N/A')}")
        print(f"   - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample.get('text', ''))}")
        print(f"   - í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {sample.get('text', '')[:200]}...")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def save_sample_data(dataset, num_samples=100):
    """
    ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    if dataset is None:
        print("âŒ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n=== ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì¤‘ ({num_samples}ê°œ) ===")
    
    try:
        # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ
        sample_data = dataset['train'].select(range(min(num_samples, len(dataset['train']))))
        
        # CSVë¡œ ì €ì¥
        sample_data.to_csv('wikipedia_ko_sample.csv')
        print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: wikipedia_ko_sample.csv")
        
        # JSONìœ¼ë¡œ ì €ì¥
        sample_data.to_json('wikipedia_ko_sample.json')
        print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: wikipedia_ko_sample.json")
        
    except Exception as e:
        print(f"âŒ ì €ì¥ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    dataset = download_wikipedia_ko()
    
    if dataset:
        # ìƒ˜í”Œ ë°ì´í„° ì €ì¥
        save_sample_data(dataset, num_samples=1000)
        
        print(f"\nğŸ‰ ì™„ë£Œ! Wikipedia í•œêµ­ì–´ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ’¡ ì‚¬ìš©ë²•:")
        print(f"   - ì „ì²´ ë°ì´í„°: dataset['train']")
        print(f"   - íŠ¹ì • ë¬¸ì„œ: dataset['train'][ì¸ë±ìŠ¤]")
        print(f"   - ìƒ˜í”Œ ë°ì´í„°: wikipedia_ko_sample.csv ë˜ëŠ” .json")
    else:
        print("âŒ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
